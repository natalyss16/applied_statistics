condition_number_stand
lm_hitters <- lm(y ~ X)
summary(lm_hitters)
lm_coefs <- as.vector(round(coef(lm_hitters), 4))
ridge_coefs <- as.vector(round(coef(ridge_hitters), 4))
variables <- c("Intercept", colnames(X))
coefs <- data.frame(
  variable = variables,
  lm_coefs = lm_coefs,
  ridge_coefs = ridge_coefs
)
coefs
lambda <- 70
ridge_hitters <- glmnet(X, y, alpha = 0, lambda = lambda)
as.data.frame(as.matrix(coef(ridge_hitters)))
lm_coefs <- as.vector(round(coef(lm_hitters), 4))
ridge_coefs <- as.vector(round(coef(ridge_hitters), 4))
variables <- c("Intercept", colnames(X))
coefs <- data.frame(
  variable = variables,
  lm_coefs = lm_coefs,
  ridge_coefs = ridge_coefs
)
coefs
lm_coefs <- as.vector(round(coef(lm_hitters), 4))
ridge_coefs <- as.vector(round(coef(ridge_hitters), 4))
variables <- c("Intercept", colnames(X))
coefs <- data.frame(
  variable = variables,
  lm_coefs = lm_coefs,
  ridge_coefs = ridge_coefs,
  diff = lm_coefs - ridge_coefs
)
coefs
.vsc.attach()
.vsc.attach()
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- seq(0:100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- seq(0:1000)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- seq(0:1000)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- seq(0:1000)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- 10^seq(10, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- 10^seq(10, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
potential_lambdas <- 10^seq(10, -2, length = 1)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
# found that this sequence is usually used to set the wide range of penalty strngth
potential_lambdas <- 10^seq(10, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
# found that this sequence is usually used to set the wide range of penalty strngth
potential_lambdas <- 10^seq(100, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
# found that this sequence is usually used to set the wide range of penalty strngth
potential_lambdas <- 10^seq(100, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
best_lambda
set.seed(1122)
# we'll take 80% train and 20% test
train_indices <- sample(1:nrow(hitters), size = 0.8 * nrow(hitters))
hitters_train <- hitters[train_indices, ]
hitters_test <- hitters[-train_indices, ]
# design matrices
X_train <- model.matrix(Salary ~ ., data = hitters_train)
y_train <- hitters_train$Salary
X_test <- model.matrix(Salary ~ ., data = hitters_test)
y_test <- hitters_test$Salary
# set seq of potential lambdas
# found that this sequence is usually used to set the wide range of penalty strngth
potential_lambdas <- 10^seq(10, -2, length = 100)
ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = potential_lambdas)
# make a prediction on test data
pred_hitters <- predict(ridge_hitters_train, newx = X_test)
# calculate MSE for each lambda from seq
mse <- colMeans((pred_hitters - y_test)^2)
best_lambda <- potential_lambdas[which.min(mse)]
mse
min(mse)
pred_hitters - y_test
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  mse
}
potential_lambdas <- 10^seq(10, -2, length = 100)
optimal_lambda(potential_lambdas)
mse_values <- sapply(potential_lambdas, optimal_lambda)
mse_values
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse_values <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse_values, type = "b", 
     xlab = "log(lambda)", ylab = "Mean Squared Error", 
     main = "MSE vs. log(lambda)")
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse, #type = "b", 
     xlab = "log(lambda)", ylab = "Mean Squared Error", 
     main = "MSE vs. log(lambda)")
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse, #type = "b", 
     xlab = "log(lambda)", ylab = "Mean Squared Error", 
     main = "MSE vs. log(lambda)")
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse)
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse,
     xlab = "log(lambda)", ylab = "MSE")
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
plot(log(potential_lambdas), mse,
     xlab = "log(lambda)", ylab = "MSE")
potential_lambdas[which.min(mse)]
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
  geom_line(color = "#3f3fb1") +
  geom_point(color = "#be3c3c") +
  geom_vline(xintercept = log(best_lambda), linetype = "dashed", color = "#3bd23b") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#3bd23b", size = 3) +
  labs(title = "MSE vs. log(lambda)", x = "log(lambda)", y = "Mean Squared Error") +
  annotate("text", x = log(best_lambda), y = min_mse, 
           label = paste("Min MSE = ", round(min_mse, 2), 
                         "\nlog(lambda) = ", round(log(best_lambda), 2)), 
           vjust = -1.5, color = "#3bd23b") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
#  geom_line(color = "#3f3fb1") +
  geom_point(color = "#be3c3c") +
#  geom_vline(xintercept = log(best_lambda), linetype = "dashed", color = "#3bd23b") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#3bd23b", size = 3) +
  labs(title = "MSE vs. log(lambda)", x = "log(lambda)", y = "Mean Squared Error") +
  annotate("text", x = log(best_lambda), y = min_mse, 
           label = paste("Min MSE = ", round(min_mse, 2), 
                         "\nlog(lambda) = ", round(log(best_lambda), 2)), 
           vjust = -1.5, color = "#3bd23b") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
#  geom_line(color = "#3f3fb1") +
  geom_point(color = "#be3c3c") +
#  geom_vline(xintercept = log(best_lambda), linetype = "dashed", color = "#3bd23b") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#3bd23b", size = 1) +
  labs(title = "MSE vs. log(lambda)", x = "log(lambda)", y = "Mean Squared Error") +
  annotate("text", x = log(best_lambda), y = min_mse, 
           label = paste("Min MSE = ", round(min_mse, 2), 
                         "\nlog(lambda) = ", round(log(best_lambda), 2)), 
           vjust = -1.5, color = "#3bd23b") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
#  geom_line(color = "#3f3fb1") +
  geom_point(color = "#be3c3c") +
#  geom_vline(xintercept = log(best_lambda), linetype = "dashed", color = "#3bd23b") +
  geom_point(aes(x = log(potential_lambdas[which.min(mse)]), y = min(mse)), color = "#3bd23b", size = 1) +
  labs(title = "MSE vs. log(lambda)", x = "log(lambda)", y = "Mean Squared Error") +
  annotate("text", x = log(best_lambda), y = min_mse, 
           label = paste("Min MSE = ", round(min_mse, 2), 
                         "\nlog(lambda) = ", round(log(best_lambda), 2)), 
           vjust = -1.5, color = "#060706") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
  geom_point(color = "#3c9dbe") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#3f703f", size = 1) +
  labs(title = "MSE & log(lambda)", x = "log(lambda)", y = "MSE") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
  geom_point(color = "#3c9dbe") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#82dc82", size = 1) +
  labs(title = "MSE & log(lambda)", x = "log(lambda)", y = "MSE") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to add text to the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
  geom_point(color = "#3c5abe") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#82dc82", size = 1) +
  labs(title = "MSE & log(lambda)", x = "log(lambda)", y = "MSE") +
  theme_minimal()
optimal_lambda <- function(lambda) {
  ridge_hitters_train <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  pred_hitters <- predict(ridge_hitters_train, newx = X_test)
  mse <- colMeans((pred_hitters - y_test)^2)
  return(mse)
}
potential_lambdas <- 10^seq(10, -2, length = 100)
mse <- sapply(potential_lambdas, optimal_lambda)
mse_df <- data.frame(
  log_lambda = log(potential_lambdas),
  mse = mse
)
# to display the needed point on the graph
best_lambda <- potential_lambdas[which.min(mse)]
min_mse <- min(mse)
ggplot(mse_df, aes(x = log_lambda, y = mse)) +
  geom_point(color = "#3c5abe") +
  geom_point(aes(x = log(best_lambda), y = min_mse), color = "#82dc82", size = 1) +
  labs(
    x = "log(lambda)",
    y = "MSE"
  ) +
  theme_minimal()
print(best_lambda, min_mse)
